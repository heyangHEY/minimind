{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 0-Tokenizer\n",
    "\n",
    "Tokenizerï¼ˆåˆ†è¯å™¨ï¼‰åœ¨ NLP é¢†åŸŸæ‰®æ¼”ç€åŸºç¡€ä¸”å…³é”®çš„ä½œç”¨ï¼Œå®ƒå°†æ–‡æœ¬åˆ†å‰²æˆå•è¯æˆ–å­è¯å¹¶è½¬åŒ–ä¸ºæ•°ç»„ç¼–å·ï¼Œä¸ºæ¨¡å‹æä¾›å¯å¤„ç†çš„è¾“å…¥ï¼Œåœ¨æ–‡æœ¬é¢„å¤„ç†ã€è¯­ä¹‰ç†è§£åŠé€‚é…ä¸åŒè¯­è¨€å’Œä»»åŠ¡ç­‰æ–¹é¢å¥ å®šåŸºç¡€ï¼Œæ˜¯è¿æ¥è‡ªç„¶è¯­è¨€æ–‡æœ¬ä¸è®¡ç®—æœºå¯å¤„ç†æ•°æ®çš„é‡è¦æ¡¥æ¢."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## å­è¯åˆ†è¯ç®—æ³•\n",
    "\n",
    "å¸¸è§çš„å­è¯åˆ†è¯ç®—æ³•æœ‰ä¸‰ç§ï¼š\n",
    "\n",
    "1. å­—èŠ‚å¯¹ç¼–ç ï¼ˆByte Pair Encodingï¼ŒBPEï¼‰\n",
    "2. WordPiece\n",
    "3. Unigram\n",
    "\n",
    "### BPE\n",
    "\n",
    "BPE æ˜¯ä¸€ç§ç®€å•çš„æ•°æ®å‹ç¼©æŠ€æœ¯ï¼Œå®ƒä¼šè¿­ä»£åœ°æ›¿æ¢åºåˆ—ä¸­æœ€é¢‘ç¹å‡ºç°çš„å­—èŠ‚å¯¹ã€‚BPE ä¾èµ–ä¸€ä¸ªé¢„åˆ†è¯å™¨ï¼Œè¯¥é¢„åˆ†è¯å™¨ä¼šå°†è®­ç»ƒæ•°æ®åˆ†å‰²æˆå•è¯ï¼ˆåœ¨æœ¬é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æŒ‰ç©ºæ ¼åˆ†è¯çš„æ–¹æ³•ä½œä¸ºé¢„åˆ†è¯æ–¹æ³•ï¼‰.\n",
    "\n",
    "åœ¨é¢„åˆ†è¯ä¹‹åï¼Œä¼šåˆ›å»ºä¸€ç»„å”¯ä¸€çš„å•è¯ï¼Œå¹¶ç¡®å®šå®ƒä»¬åœ¨æ•°æ®ä¸­çš„å‡ºç°é¢‘ç‡ã€‚æ¥ä¸‹æ¥ï¼ŒBPE ä¼šåˆ›å»ºä¸€ä¸ªåŸºç¡€è¯è¡¨ï¼Œè¯¥è¯è¡¨åŒ…å«é¢„åˆ†è¯å™¨æœ€åˆç”Ÿæˆçš„æ•°æ®ä¸­æ‰€æœ‰å”¯ä¸€å•è¯çš„ç¬¦å·ã€‚ç„¶åï¼Œä¼šå°†è¿™å¯¹ç¬¦å·ä»è¯è¡¨ä¸­ç§»é™¤ï¼Œæ–°å½¢æˆçš„ç¬¦å·å°†åŠ å…¥è¯è¡¨ã€‚åœ¨è¿­ä»£è¿‡ç¨‹ä¸­ï¼ŒBPE ç®—æ³•ä¼šåˆå¹¶é¢‘ç¹å‡ºç°çš„ç¬¦å·å¯¹.\n",
    "\n",
    "ç»™å®šè¯è¡¨çš„å¤§å°ï¼ŒBPEï¼ˆå­—èŠ‚å¯¹ç¼–ç ï¼‰ç®—æ³•æœ€ç»ˆä¼šåˆå¹¶å‡ºç°é¢‘ç‡æœ€é«˜çš„ç¬¦å·å¯¹ï¼Œç›´åˆ°æ”¶æ•›åˆ°è¯¥å¤§å°.\n",
    "\n",
    "### WordPiece\n",
    "\n",
    "WordPiece ç®—æ³•ä¸ BPE éå¸¸ç›¸ä¼¼ã€‚WordPiece é¦–å…ˆå°†è¯è¡¨åˆå§‹åŒ–ä¸ºåŒ…å«è®­ç»ƒæ•°æ®ä¸­å‡ºç°çš„æ¯ä¸ªå­—ç¬¦ï¼Œç„¶åé€æ­¥å­¦ä¹ ç»™å®šæ•°é‡çš„åˆå¹¶è§„åˆ™. ä¸ BPE ä¸åŒçš„æ˜¯ï¼ŒWordPiece å¹¶ä¸é€‰æ‹©æœ€é¢‘ç¹å‡ºç°çš„ç¬¦å·å¯¹ï¼Œè€Œæ˜¯é€‰æ‹©é‚£ä¸ªåŠ å…¥è¯è¡¨åèƒ½ä½¿è®­ç»ƒæ•°æ®å‡ºç°çš„å¯èƒ½æ€§æœ€å¤§åŒ–çš„ç¬¦å·å¯¹.\n",
    "\n",
    "### Unigram\n",
    "\n",
    "Unigram ç®—æ³•å°†å…¶åŸºç¡€è¯è¡¨åˆå§‹åŒ–ä¸ºå¤§é‡çš„ç¬¦å·ï¼Œç„¶åé€æ­¥å‰Šå‡æ¯ä¸ªç¬¦å·ï¼Œä»¥è·å¾—ä¸€ä¸ªæ›´å°çš„è¯è¡¨ã€‚å®ƒä¼šåœ¨è®­ç»ƒæ•°æ®ä¸Šå®šä¹‰ä¸€ä¸ªå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼Œä»¥æ­¤æ¥ç¡®å®šæ˜¯å¦ä»è¯è¡¨ä¸­ç§»é™¤æŸä¸ªç¬¦å·."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®­ç»ƒä¸€ä¸ªæœ€ç®€å•çš„åˆ†è¯å™¨\n",
    "\n",
    "åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ åŸºäº transformers åº“æ¥è®­ç»ƒä½ è‡ªå·±çš„åˆ†è¯å™¨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆå§‹åŒ–\n",
    "\n",
    "é¦–å…ˆï¼Œæˆ‘ä»¬åº”è¯¥åˆå§‹åŒ–æˆ‘ä»¬çš„åˆ†è¯å™¨ï¼Œå¹¶ç¡®å®šé€‰æ‹©å“ªç§æ–¹æ³•ã€‚æˆ‘ä»¬å°†ä½¿ç”¨å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰ç®—æ³•."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', 'Â¡', 'Â¢', 'Â£', 'Â¤', 'Â¥', 'Â¦', 'Â§', 'Â¨', 'Â©', 'Âª', 'Â«', 'Â¬', 'Â®', 'Â¯', 'Â°', 'Â±', 'Â²', 'Â³', 'Â´', 'Âµ', 'Â¶', 'Â·', 'Â¸', 'Â¹', 'Âº', 'Â»', 'Â¼', 'Â½', 'Â¾', 'Â¿', 'Ã€', 'Ã', 'Ã‚', 'Ãƒ', 'Ã„', 'Ã…', 'Ã†', 'Ã‡', 'Ãˆ', 'Ã‰', 'ÃŠ', 'Ã‹', 'ÃŒ', 'Ã', 'Ã', 'Ã', 'Ã', 'Ã‘', 'Ã’', 'Ã“', 'Ã”', 'Ã•', 'Ã–', 'Ã—', 'Ã˜', 'Ã™', 'Ãš', 'Ã›', 'Ãœ', 'Ã', 'Ã', 'ÃŸ', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã¥', 'Ã¦', 'Ã§', 'Ã¨', 'Ã©', 'Ãª', 'Ã«', 'Ã¬', 'Ã­', 'Ã®', 'Ã¯', 'Ã°', 'Ã±', 'Ã²', 'Ã³', 'Ã´', 'Ãµ', 'Ã¶', 'Ã·', 'Ã¸', 'Ã¹', 'Ãº', 'Ã»', 'Ã¼', 'Ã½', 'Ã¾', 'Ã¿', 'Ä€', 'Ä', 'Ä‚', 'Äƒ', 'Ä„', 'Ä…', 'Ä†', 'Ä‡', 'Äˆ', 'Ä‰', 'ÄŠ', 'Ä‹', 'ÄŒ', 'Ä', 'Ä', 'Ä', 'Ä', 'Ä‘', 'Ä’', 'Ä“', 'Ä”', 'Ä•', 'Ä–', 'Ä—', 'Ä˜', 'Ä™', 'Äš', 'Ä›', 'Äœ', 'Ä', 'Ä', 'ÄŸ', 'Ä ', 'Ä¡', 'Ä¢', 'Ä£', 'Ä¤', 'Ä¥', 'Ä¦', 'Ä§', 'Ä¨', 'Ä©', 'Äª', 'Ä«', 'Ä¬', 'Ä­', 'Ä®', 'Ä¯', 'Ä°', 'Ä±', 'Ä²', 'Ä³', 'Ä´', 'Äµ', 'Ä¶', 'Ä·', 'Ä¸', 'Ä¹', 'Äº', 'Ä»', 'Ä¼', 'Ä½', 'Ä¾', 'Ä¿', 'Å€', 'Å', 'Å‚', 'Åƒ']\n",
      "[33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323]\n",
      "['0x21', '0x22', '0x23', '0x24', '0x25', '0x26', '0x27', '0x28', '0x29', '0x2a', '0x2b', '0x2c', '0x2d', '0x2e', '0x2f', '0x30', '0x31', '0x32', '0x33', '0x34', '0x35', '0x36', '0x37', '0x38', '0x39', '0x3a', '0x3b', '0x3c', '0x3d', '0x3e', '0x3f', '0x40', '0x41', '0x42', '0x43', '0x44', '0x45', '0x46', '0x47', '0x48', '0x49', '0x4a', '0x4b', '0x4c', '0x4d', '0x4e', '0x4f', '0x50', '0x51', '0x52', '0x53', '0x54', '0x55', '0x56', '0x57', '0x58', '0x59', '0x5a', '0x5b', '0x5c', '0x5d', '0x5e', '0x5f', '0x60', '0x61', '0x62', '0x63', '0x64', '0x65', '0x66', '0x67', '0x68', '0x69', '0x6a', '0x6b', '0x6c', '0x6d', '0x6e', '0x6f', '0x70', '0x71', '0x72', '0x73', '0x74', '0x75', '0x76', '0x77', '0x78', '0x79', '0x7a', '0x7b', '0x7c', '0x7d', '0x7e', '0xa1', '0xa2', '0xa3', '0xa4', '0xa5', '0xa6', '0xa7', '0xa8', '0xa9', '0xaa', '0xab', '0xac', '0xae', '0xaf', '0xb0', '0xb1', '0xb2', '0xb3', '0xb4', '0xb5', '0xb6', '0xb7', '0xb8', '0xb9', '0xba', '0xbb', '0xbc', '0xbd', '0xbe', '0xbf', '0xc0', '0xc1', '0xc2', '0xc3', '0xc4', '0xc5', '0xc6', '0xc7', '0xc8', '0xc9', '0xca', '0xcb', '0xcc', '0xcd', '0xce', '0xcf', '0xd0', '0xd1', '0xd2', '0xd3', '0xd4', '0xd5', '0xd6', '0xd7', '0xd8', '0xd9', '0xda', '0xdb', '0xdc', '0xdd', '0xde', '0xdf', '0xe0', '0xe1', '0xe2', '0xe3', '0xe4', '0xe5', '0xe6', '0xe7', '0xe8', '0xe9', '0xea', '0xeb', '0xec', '0xed', '0xee', '0xef', '0xf0', '0xf1', '0xf2', '0xf3', '0xf4', '0xf5', '0xf6', '0xf7', '0xf8', '0xf9', '0xfa', '0xfb', '0xfc', '0xfd', '0xfe', '0xff', '0x100', '0x101', '0x102', '0x103', '0x104', '0x105', '0x106', '0x107', '0x108', '0x109', '0x10a', '0x10b', '0x10c', '0x10d', '0x10e', '0x10f', '0x110', '0x111', '0x112', '0x113', '0x114', '0x115', '0x116', '0x117', '0x118', '0x119', '0x11a', '0x11b', '0x11c', '0x11d', '0x11e', '0x11f', '0x120', '0x121', '0x122', '0x123', '0x124', '0x125', '0x126', '0x127', '0x128', '0x129', '0x12a', '0x12b', '0x12c', '0x12d', '0x12e', '0x12f', '0x130', '0x131', '0x132', '0x133', '0x134', '0x135', '0x136', '0x137', '0x138', '0x139', '0x13a', '0x13b', '0x13c', '0x13d', '0x13e', '0x13f', '0x140', '0x141', '0x142', '0x143']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import pre_tokenizers\n",
    "\n",
    "alphabet = pre_tokenizers.ByteLevel.alphabet()\n",
    "alphabet.sort()\n",
    "print(alphabet)\n",
    "print([ord(i) for i in alphabet])\n",
    "print([hex(ord(i)) for i in alphabet])\n",
    "# ! ä¸€å…±256ä¸ªå­—ç¬¦ï¼Œä»33å¼€å§‹ã€‚å› ä¸ºasciiè¡¨ä¸­å‰32ä¸ªå­—ç¬¦æ˜¯æ§åˆ¶å­—ç¬¦ï¼Œæ‰€ä»¥ä»33å¼€å§‹ã€‚\n",
    "# asciiæœ‰128ä¸ªä»£ç ç‚¹ï¼Œunicodeçš„å‰128ä¸ªä»£ç ç‚¹ä¸asciiç›¸åŒã€‚\n",
    "# ord() è·å–å­—ç¬¦çš„ ASCII ç \n",
    "# hex() å°† ASCII ç è½¬æ¢ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å®šä¹‰ç‰¹æ®Šæ ‡è®°\n",
    "\n",
    "æ•°æ®é›†ä¸­å­˜åœ¨ä¸€äº›æˆ‘ä»¬ä¸å¸Œæœ›è¢«åˆ†è¯çš„ç‰¹æ®Šæ ‡è®°ï¼Œæˆ‘ä»¬ä¼šå°†è¿™äº›æ ‡è®°å®šä¹‰ä¸ºç‰¹æ®Šæ ‡è®°ï¼Œå¹¶å°†å®ƒä»¬ä¼ é€’ç»™åˆ†è¯å™¨è®­ç»ƒå™¨ï¼Œä»¥é˜²æ­¢å‡ºç°é”™è¯¯çš„åˆ†è¯æƒ…å†µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"<unk>\", \"<s>\", \"</s>\"]\n",
    "\n",
    "trainer = trainers.BpeTrainer(\n",
    "        vocab_size=270, # 256 + 14\n",
    "        special_tokens=special_tokens,\n",
    "        show_progress=True,\n",
    "        initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä»æ–‡ä»¶ä¸­è¯»å–æ•°æ®\n",
    "\n",
    "åœ¨æœ¬æ¬¡å®éªŒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ JSON Linesï¼ˆjsonlï¼‰æ ¼å¼æ¥å­˜å‚¨ Tokenizer è®­ç»ƒæ•°æ®ï¼Œåˆ†è¯å™¨å†…ç½®çš„è®­ç»ƒå‡½æ•°è¦æ±‚è®­ç»ƒæ•°æ®ä»¥è¿­ä»£å™¨çš„å½¢å¼ä¼ å…¥ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆè·å–ä¸€ä¸ªæ•°æ®è¯»å–çš„ç”Ÿæˆå™¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_texts_from_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            yield data['text']\n",
    "\n",
    "data_path = './toydata/tokenizer_data.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1: <s>è¿‘å¹´æ¥ï¼Œäººå·¥æ™ºèƒ½æŠ€æœ¯è¿…é€Ÿå‘å±•ï¼Œæ·±åˆ»æ”¹å˜äº†å„è¡Œå„ä¸šçš„é¢è²Œã€‚æœºå™¨å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸçš„çªç ´æ€§è¿›å±•ï¼Œä½¿å¾—æ™ºèƒ½äº§å“å’ŒæœåŠ¡è¶Šæ¥è¶Šæ™®åŠã€‚ä»æ™ºèƒ½å®¶å±…åˆ°è‡ªåŠ¨é©¾é©¶ï¼Œå†åˆ°æ™ºèƒ½åŒ»ç–—ï¼ŒAIçš„åº”ç”¨åœºæ™¯æ­£åœ¨å¿«é€Ÿæ‹“å±•ã€‚éšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œæœªæ¥çš„äººå·¥æ™ºèƒ½å°†æ›´åŠ æ™ºèƒ½ã€æ›´åŠ è´´è¿‘äººç±»ç”Ÿæ´»ã€‚</s>\n"
     ]
    }
   ],
   "source": [
    "data_iter = read_texts_from_jsonl(data_path)\n",
    "print(f'Row 1: {next(data_iter)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¼€å§‹è®­ç»ƒ!\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨åˆ†è¯å™¨çš„å†…ç½®å‡½æ•° `tokenizer.train_from_iterator` æ¥è®­ç»ƒåˆ†è¯å™¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(data_iter, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è®¾ç½®è§£ç å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥ï¼Œæ£€æŸ¥ä¸€ä¸‹ç‰¹æ®Šæ ‡è®°æ˜¯å¦å¾—åˆ°äº†å¦¥å–„å¤„ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.token_to_id('<unk>') == 0\n",
    "assert tokenizer.token_to_id('<s>') == 1\n",
    "assert tokenizer.token_to_id('</s>') == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å°†è®­ç»ƒå¥½çš„åˆ†è¯å™¨ä¿å­˜åˆ°ç£ç›˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/toy_tokenizer/vocab.json', './model/toy_tokenizer/merges.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "tokenizer_dir = \"./model/toy_tokenizer\"\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "tokenizer.save(os.path.join(tokenizer_dir, \"tokenizer.json\")) # At this point, you will see a file named tokenizer.json under tokenizer_dir\n",
    "tokenizer.model.save(tokenizer_dir) # generate vocab.json & merges.txt\n",
    "\n",
    "# vocab_size=256+3+11=270ï¼Œç»™ç‰¹æ®Šæ ‡è®°3ä¸ªï¼Œç»™è¯­æ–™ä¸­çš„pair 11ä¸ª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ‰‹åŠ¨åˆ›å»ºä¸€ä»½é…ç½®æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer training completed and saved.\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"add_bos_token\": False,\n",
    "    \"add_eos_token\": False,\n",
    "    \"add_prefix_space\": False,\n",
    "    \"added_tokens_decoder\": {\n",
    "        \"0\": {\n",
    "            \"content\": \"<unk>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "        },\n",
    "        \"1\": {\n",
    "            \"content\": \"<s>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "        },\n",
    "        \"2\": {\n",
    "            \"content\": \"</s>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "        }\n",
    "    },\n",
    "    \"additional_special_tokens\": [],\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"clean_up_tokenization_spaces\": False,\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"legacy\": True,\n",
    "    \"model_max_length\": 32768,\n",
    "    \"pad_token\": \"<unk>\",\n",
    "    \"sp_model_kwargs\": {},\n",
    "    \"spaces_between_special_tokens\": False,\n",
    "    \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"chat_template\": \"{{ '<s>' + messages[0]['text'] + '</s>' }}\"\n",
    "}\n",
    "\n",
    "with open(os.path.join(tokenizer_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as config_file:\n",
    "    json.dump(config, config_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tokenizer training completed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨æˆ‘ä»¬å·²ç»è®­ç»ƒäº†ä¸€ä¸ªç®€å•çš„åˆ†è¯å™¨ï¼Œå¹¶å°†å…¶è¿›è¡Œä¿å­˜ï¼Œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¯•ç€åŠ è½½å®ƒï¼Œå¹¶ä½¿ç”¨å…¶å¸®åŠ©æˆ‘ä»¬å¯¹æ–‡æœ¬è¿›è¡Œç¼–è§£ç ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hey/anaconda3/envs/minimind/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹æ–‡æœ¬ï¼š[{'text': 'å¤±å»çš„ä¸œè¥¿å°±è¦å­¦ç€å»æ¥å—ï¼Œå­¦ç€æ”¾ä¸‹ã€‚'}]\n",
      "ä¿®æ”¹æ–‡æœ¬ï¼š[{'text': 'å¤±å»çš„ä¸œè¥¿å°±è¦å­¦ç€å»æ¥å—ï¼Œå­¦ç€æ”¾ä¸‹ã€‚'}] (æ·»åŠ è‡ªå®šä¹‰èŠå¤©æ¨¡æ¿)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./model/toy_tokenizer\")\n",
    "msg = [{\"text\": \"å¤±å»çš„ä¸œè¥¿å°±è¦å­¦ç€å»æ¥å—ï¼Œå­¦ç€æ”¾ä¸‹ã€‚\"}]\n",
    "new_msg = tokenizer.apply_chat_template(\n",
    "    msg,\n",
    "    tokenize=False\n",
    ")\n",
    "print(f'åŸå§‹æ–‡æœ¬ï¼š{msg}')\n",
    "print(f'ä¿®æ”¹æ–‡æœ¬ï¼š{msg} (æ·»åŠ è‡ªå®šä¹‰èŠå¤©æ¨¡æ¿)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆ†è¯å™¨è¯è¡¨å¤§å°ï¼š270\n"
     ]
    }
   ],
   "source": [
    "print(f'åˆ†è¯å™¨è¯è¡¨å¤§å°ï¼š{tokenizer.vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æŸ¥çœ‹åˆ†è¯ç»“æœï¼š\n",
      "{'input_ids': [1, 260, 112, 164, 239, 122, 265, 259, 253, 167, 101, 126, 164, 111, 112, 167, 102, 226, 164, 258, 102, 166, 254, 225, 164, 239, 122, 165, 239, 101, 266, 248, 264, 164, 258, 102, 166, 254, 225, 165, 245, 125, 259, 236, 269, 227, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer(new_msg)\n",
    "print(f'æŸ¥çœ‹åˆ†è¯ç»“æœï¼š\\n{model_inputs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¯¹åˆ†è¯ç»“æœè¿›è¡Œè§£ç ï¼š<s>å¤±å»çš„ä¸œè¥¿å°±è¦å­¦ç€å»æ¥å—ï¼Œå­¦ç€æ”¾ä¸‹ã€‚</s> (ä¿ç•™ç‰¹æ®Šå­—ç¬¦)\n"
     ]
    }
   ],
   "source": [
    "response = tokenizer.decode(model_inputs['input_ids'], skip_special_tokens=False)\n",
    "print(f'å¯¹åˆ†è¯ç»“æœè¿›è¡Œè§£ç ï¼š{response} (ä¿ç•™ç‰¹æ®Šå­—ç¬¦)' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¯¹åˆ†è¯ç»“æœè¿›è¡Œè§£ç ï¼šå¤±å»çš„ä¸œè¥¿å°±è¦å­¦ç€å»æ¥å—ï¼Œå­¦ç€æ”¾ä¸‹ã€‚ (ç§»é™¤ç‰¹æ®Šå­—ç¬¦)\n"
     ]
    }
   ],
   "source": [
    "response = tokenizer.decode(model_inputs['input_ids'], skip_special_tokens=True)\n",
    "print(f'å¯¹åˆ†è¯ç»“æœè¿›è¡Œè§£ç ï¼š{response} (ç§»é™¤ç‰¹æ®Šå­—ç¬¦)' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‚è€ƒèµ„æ–™\n",
    "\n",
    "- [Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/zh-CN/chapter2/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8/tree/main\n",
    "\n",
    "Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8\n",
    "\n",
    "tokenizer_config.json\n",
    "```json\n",
    "\"model_max_length\": 131072,\n",
    "chat_template=\"\n",
    "{%- if tools %}\n",
    "    {{- '<|im_start|>system\\\\n' }}\n",
    "    {%- if messages[0]['role'] == 'system' %}\n",
    "        {{- messages[0]['content'] }}\n",
    "    {%- else %}\n",
    "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
    "    {%- endif %}\n",
    "    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\n",
    "    {%- for tool in tools %}\n",
    "        {{- \\\"\\\\n\\\" }}\n",
    "        {{- tool | tojson }}\n",
    "    {%- endfor %}\n",
    "    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    \n",
    "    {%- if messages[0]['role'] == 'system' %}\n",
    "        {{- '<|im_start|>system\\\\n' + messages[0]['content'] + '<|im_end|>\\\\n' }}\n",
    "    {%- else %}\n",
    "        {{- '<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n' }}\n",
    "    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\n",
    "    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) or (message.role == \\\"assistant\\\" and not message.tool_calls) %}\n",
    "        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\n",
    "    {%- elif message.role == \\\"assistant\\\" %}\n",
    "        {{- '<|im_start|>' + message.role }}\n",
    "    {%- if message.content %}\n",
    "        {{- '\\\\n' + message.content }}\n",
    "    {%- endif %}\n",
    "    {%- for tool_call in message.tool_calls %}\n",
    "        {%- if tool_call.function is defined %}\n",
    "            {%- set tool_call = tool_call.function %}\n",
    "        {%- endif %}\n",
    "        {{- '\\\\n<tool_call>\\\\n{\\\"name\\\": \\\"' }}\n",
    "        {{- tool_call.name }}\n",
    "        {{- '\\\", \\\"arguments\\\": ' }}\n",
    "        {{- tool_call.arguments | tojson }}\n",
    "        {{- '}\\\\n</tool_call>' }}\n",
    "    {%- endfor %}\n",
    "    {{- '<|im_end|>\\\\n' }}\n",
    "    {%- elif message.role == \\\"tool\\\" %}\n",
    "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\n",
    "            {{- '<|im_start|>user' }}\n",
    "        {%- endif %}\n",
    "        {{- '\\\\n<tool_response>\\\\n' }}\n",
    "        {{- message.content }}\n",
    "        {{- '\\\\n</tool_response>' }}   \n",
    "        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}        \n",
    "            {{- '<|im_end|>\\\\n' }}    \n",
    "        {%- endif %}\n",
    "    {%- endif %}\n",
    "{%- endfor %}\n",
    "{%- if add_generation_prompt %}  \n",
    "    {{- '<|im_start|>assistant\\\\n' }}\n",
    "{%- endif %}\n",
    "   \",\n",
    "```\n",
    "æ¨¡æ¿çš„ç»“æ„åˆ†ä¸ºå‡ ä¸ªéƒ¨åˆ†ã€‚æœ€å¤–å±‚æ£€æŸ¥æ˜¯å¦æœ‰ tools å˜é‡å­˜åœ¨ã€‚å¦‚æœæœ‰çš„è¯ï¼Œç”Ÿæˆç³»ç»Ÿæ¶ˆæ¯ï¼Œå¹¶åŒ…å«å·¥å…·çš„ä¿¡æ¯ã€‚å¦‚æœæ²¡æœ‰ï¼Œåˆ™æ ¹æ® messages é‡Œçš„ç¬¬ä¸€ä¸ªæ¶ˆæ¯æ˜¯å¦ä¸ºç³»ç»Ÿæ¶ˆæ¯æ¥å¤„ç†ã€‚æ¥ç€éå† messages æ•°ç»„ï¼Œæ ¹æ®ä¸åŒçš„è§’è‰²ï¼ˆuserã€assistantã€toolï¼‰ç”Ÿæˆå¯¹åº”çš„éƒ¨åˆ†ã€‚æœ€åå¦‚æœæœ‰ç”Ÿæˆæç¤ºçš„è¯ï¼Œæ·»åŠ  assistant çš„å¼€å¤´ã€‚\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hey/anaconda3/envs/minimind/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='../model/minimind_tokenizer', vocab_size=6400, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "['<s>', '</s>', '<unk>']\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../model/minimind_tokenizer\")\n",
    "print(tokenizer)\n",
    "print(tokenizer.all_special_tokens)\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ã¥Â¤Â±', 'Ã¥Ä°Â»', 'Ã§Ä¼Ä¦', 'Ã¤Â¸Ä¾', 'Ã¨Â¥Â¿', 'Ã¥Â°Â±', 'Ã¨Â¦Ä£', 'Ã¥ÅƒÂ¦', 'Ã§Ä¿Ä¢', 'Ã¥Ä°Â»', 'Ã¦Ä°Â¥Ã¥Ä±Ä¹', 'Ã¯Â¼Ä®', 'Ã¥ÅƒÂ¦', 'Ã§Ä¿Ä¢', 'Ã¦Ä¶Â¾', 'Ã¤Â¸Ä­', 'Ã£Ä¢Ä¤']\n"
     ]
    }
   ],
   "source": [
    "msg = \"å¤±å»çš„ä¸œè¥¿å°±è¦å­¦ç€å»æ¥å—ï¼Œå­¦ç€æ”¾ä¸‹ã€‚\"\n",
    "tokens = tokenizer.tokenize(msg)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2262, 1473, 269, 3600, 2540, 1343, 404, 595, 978, 1473, 4926, 270, 595, 978, 1192, 572, 286]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤±å»çš„ä¸œè¥¿å°±è¦å­¦ç€å»æ¥å—ï¼Œå­¦ç€æ”¾ä¸‹ã€‚\n"
     ]
    }
   ],
   "source": [
    "decoded_str = tokenizer.decode(ids)\n",
    "print(decoded_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{{ '<s>system\\\\n' + system_message + '</s>\\\\n' }}{% else %}{{ '<s>system\\\\nä½ æ˜¯ MiniMindï¼Œæ˜¯ä¸€ä¸ªæœ‰ç”¨çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚</s>\\\\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<s>user\\\\n' + content + '</s>\\\\n<s>assistant\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '</s>' + '\\\\n' }}{% endif %}{% endfor %}\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model/minimind_tokenizer/tokenizer_config.json\n",
    "\n",
    "chat_template=\"\n",
    "    {% if messages[0]['role'] == 'system' %}\n",
    "        {% set system_message = messages[0]['content'] %}\n",
    "        {{ '<s>system\\\\n' + system_message + '</s>\\\\n' }}\n",
    "    {% else %}\n",
    "        {{ '<s>system\\\\nä½ æ˜¯ MiniMindï¼Œæ˜¯ä¸€ä¸ªæœ‰ç”¨çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚</s>\\\\n' }}\n",
    "    {% endif %}\n",
    "    {% for message in messages %}\n",
    "        {% set content = message['content'] %}\n",
    "        {% if message['role'] == 'user' %}\n",
    "            {{ '<s>user\\\\n' + content + '</s>\\\\n<s>assistant\\\\n' }}\n",
    "        {% elif message['role'] == 'assistant' %}\n",
    "            {{ content + '</s>' + '\\\\n' }}\n",
    "        {% endif %}\n",
    "    {% endfor %}\n",
    "\"\n",
    "\n",
    "<s>system\n",
    "ä½ æ˜¯ MiniMindï¼Œæ˜¯ä¸€ä¸ªæœ‰ç”¨çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚</s>\n",
    "<s>user\n",
    "content</s>\n",
    "<s>assistant\n",
    "content</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>system\n",
      "ä½ æ˜¯ xxï¼Œæ˜¯ä¸€ä¸ªæœ‰ç”¨çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚</s>\n",
      "\n",
      "system\n",
      "ä½ æ˜¯ xxï¼Œæ˜¯ä¸€ä¸ªæœ‰ç”¨çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚\n",
      "\n",
      "<s>system\n",
      "ä½ æ˜¯ xxï¼Œæ˜¯ä¸€ä¸ªæœ‰ç”¨çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨chatæ¨¡æ¿\n",
    "msg = [{\"role\": \"system\", \"content\": \"ä½ æ˜¯ xxï¼Œæ˜¯ä¸€ä¸ªæœ‰ç”¨çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚\"}]\n",
    "new_msg = tokenizer.apply_chat_template(\n",
    "    msg,\n",
    "    tokenize=False\n",
    ")\n",
    "print(new_msg)\n",
    "\n",
    "tokens = tokenizer.tokenize(new_msg)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# ç§»é™¤ç‰¹æ®Šå­—ç¬¦ # æ¯”å¦‚åœ¨è¿™ä¸ªtokenizerä¸­ï¼Œ<s>ã€</s>ã€<unk> æ˜¯ç‰¹æ®Šå­—ç¬¦\n",
    "decoded_str = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "print(decoded_str)\n",
    "\n",
    "# ä¿ç•™ç‰¹æ®Šå­—ç¬¦\n",
    "decoded_str = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "print(decoded_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='../model/minimind_tokenizer', vocab_size=6400, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "all_special_tokens:  ['<s>', '</s>', '<unk>']\n",
      "special_tokens_map:  {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}\n",
      "['Ã¥Â¤Â±', 'Ã¥Ä°Â»', 'Ã§Ä¼Ä¦', 'Ã¤Â¸Ä¾', 'Ã¨Â¥Â¿', 'Ã¥Â°Â±', 'Ã¨Â¦Ä£', 'Ã¥ÅƒÂ¦', 'Ã§Ä¿Ä¢', 'Ã¥Ä°Â»', 'Ã¦Ä°Â¥Ã¥Ä±Ä¹', 'Ã¯Â¼Ä®', 'Ã¥ÅƒÂ¦', 'Ã§Ä¿Ä¢', 'Ã¦Ä¶Â¾', 'Ã¤Â¸Ä­', 'Ã£Ä¢Ä¤']\n",
      "['Ã¥Â¤Â±', 'Ã¥Ä°Â»', 'Ã§Ä¼Ä¦', 'Ã¤Â¸Ä¾', 'Ã¨Â¥Â¿', 'Ã¥Â°Â±', 'Ã¨Â¦Ä£', 'Ã¥ÅƒÂ¦', 'Ã§Ä¿Ä¢', 'Ã¥Ä°Â»', 'Ã¦Ä°Â¥Ã¥Ä±Ä¹', 'Ã¯Â¼Ä®', 'Ã¥ÅƒÂ¦', 'Ã§Ä¿Ä¢', 'Ã¦Ä¶Â¾', 'Ã¤Â¸Ä­', 'Ã£Ä¢Ä¤']\n",
      "['Ã°', 'Å', 'Ä³', 'Ä¯']\n",
      "['Ã°', 'Å', 'Ä³', 'Ä¯', 'Ã°', 'Å', 'Ä±', 'Â»']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../model/minimind_tokenizer\")\n",
    "print(tokenizer)\n",
    "print(\"all_special_tokens: \", tokenizer.all_special_tokens)\n",
    "print(\"special_tokens_map: \", tokenizer.special_tokens_map)\n",
    "\n",
    "msg = \"å¤±å»çš„ä¸œè¥¿å°±è¦å­¦ç€å»æ¥å—ï¼Œå­¦ç€æ”¾ä¸‹ã€‚\"\n",
    "tokens = tokenizer.tokenize(msg, add_special_tokens=True)\n",
    "print(tokens)\n",
    "tokens = tokenizer.tokenize(msg, add_special_tokens=False)\n",
    "print(tokens)\n",
    "# add_special_tokens=True ä¼šæ·»åŠ ç‰¹æ®Šå­—ç¬¦ï¼Œadd_special_tokens=False ä¸ä¼šæ·»åŠ ç‰¹æ®Šå­—ç¬¦\n",
    "# TODO æ²¡æ•ˆæœï¼Œä¸ºä½•æ²¡åŠ  <s> å’Œ </s> å‘¢ï¼Ÿ\n",
    "\n",
    "print(tokenizer.tokenize(\"ğŸ‘\")) # BBPE ä¼šæŠŠ emoji æ‹†åˆ†æˆå­—èŠ‚ï¼Œæ‰€ä»¥åº”è¯¥æ˜¯ä¸ä¼šæœ‰oovå’Œ<unk>çš„\n",
    "print(tokenizer.tokenize(\"ğŸ‘ğŸ»\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "all_special_tokens:  ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
      "special_tokens_map:  {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "['[CLS]', 'hello', ',', 'world', ',', '[UNK]', '[UNK]', '[UNK]', 'æ–‡', 'è¥¿', '!', '[SEP]']\n",
      "['hello', ',', 'world', ',', '[UNK]', '[UNK]', '[UNK]', 'æ–‡', 'è¥¿', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# OSError: We couldn't connect to 'https://huggingface.co' to load this file\n",
    "# éœ€è¦è®¾ç½® HF_ENDPOINT=https://hf-mirror.com\n",
    "\n",
    "print(tokenizer)\n",
    "print(\"all_special_tokens: \", tokenizer.all_special_tokens)\n",
    "print(\"special_tokens_map: \", tokenizer.special_tokens_map)\n",
    "\n",
    "msg = \"hello, world, æˆ‘æ˜¯è¾¾æ–‡è¥¿!\"\n",
    "tokens = tokenizer.tokenize(msg, add_special_tokens=True)\n",
    "print(tokens)\n",
    "tokens = tokenizer.tokenize(msg, add_special_tokens=False)\n",
    "print(tokens)\n",
    "# add_special_tokens æœ‰æ•ˆæœäº†ï¼Œåœ¨tokenizationæ—¶é¦–å°¾æ·»åŠ äº† [CLS] å’Œ [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
