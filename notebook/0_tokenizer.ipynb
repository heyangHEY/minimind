{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 0-Tokenizer\n",
    "\n",
    "Tokenizer（分词器）在 NLP 领域扮演着基础且关键的作用，它将文本分割成单词或子词并转化为数组编号，为模型提供可处理的输入，在文本预处理、语义理解及适配不同语言和任务等方面奠定基础，是连接自然语言文本与计算机可处理数据的重要桥梁."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 子词分词算法\n",
    "\n",
    "常见的子词分词算法有三种：\n",
    "\n",
    "1. 字节对编码（Byte Pair Encoding，BPE）\n",
    "2. WordPiece\n",
    "3. Unigram\n",
    "\n",
    "### BPE\n",
    "\n",
    "BPE 是一种简单的数据压缩技术，它会迭代地替换序列中最频繁出现的字节对。BPE 依赖一个预分词器，该预分词器会将训练数据分割成单词（在本项目中，我们使用按空格分词的方法作为预分词方法）.\n",
    "\n",
    "在预分词之后，会创建一组唯一的单词，并确定它们在数据中的出现频率。接下来，BPE 会创建一个基础词表，该词表包含预分词器最初生成的数据中所有唯一单词的符号。然后，会将这对符号从词表中移除，新形成的符号将加入词表。在迭代过程中，BPE 算法会合并频繁出现的符号对.\n",
    "\n",
    "给定词表的大小，BPE（字节对编码）算法最终会合并出现频率最高的符号对，直到收敛到该大小.\n",
    "\n",
    "### WordPiece\n",
    "\n",
    "WordPiece 算法与 BPE 非常相似。WordPiece 首先将词表初始化为包含训练数据中出现的每个字符，然后逐步学习给定数量的合并规则. 与 BPE 不同的是，WordPiece 并不选择最频繁出现的符号对，而是选择那个加入词表后能使训练数据出现的可能性最大化的符号对.\n",
    "\n",
    "### Unigram\n",
    "\n",
    "Unigram 算法将其基础词表初始化为大量的符号，然后逐步削减每个符号，以获得一个更小的词表。它会在训练数据上定义一个对数似然损失，以此来确定是否从词表中移除某个符号."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练一个最简单的分词器\n",
    "\n",
    "在本节中，我们将学习基于 transformers 库来训练你自己的分词器."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化\n",
    "\n",
    "首先，我们应该初始化我们的分词器，并确定选择哪种方法。我们将使用字节对编码（BPE）算法."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¡', '¢', '£', '¤', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '®', '¯', '°', '±', '²', '³', '´', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', 'À', 'Á', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ï', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ø', 'Ù', 'Ú', 'Û', 'Ü', 'Ý', 'Þ', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', '÷', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'þ', 'ÿ', 'Ā', 'ā', 'Ă', 'ă', 'Ą', 'ą', 'Ć', 'ć', 'Ĉ', 'ĉ', 'Ċ', 'ċ', 'Č', 'č', 'Ď', 'ď', 'Đ', 'đ', 'Ē', 'ē', 'Ĕ', 'ĕ', 'Ė', 'ė', 'Ę', 'ę', 'Ě', 'ě', 'Ĝ', 'ĝ', 'Ğ', 'ğ', 'Ġ', 'ġ', 'Ģ', 'ģ', 'Ĥ', 'ĥ', 'Ħ', 'ħ', 'Ĩ', 'ĩ', 'Ī', 'ī', 'Ĭ', 'ĭ', 'Į', 'į', 'İ', 'ı', 'Ĳ', 'ĳ', 'Ĵ', 'ĵ', 'Ķ', 'ķ', 'ĸ', 'Ĺ', 'ĺ', 'Ļ', 'ļ', 'Ľ', 'ľ', 'Ŀ', 'ŀ', 'Ł', 'ł', 'Ń']\n",
      "[33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323]\n",
      "['0x21', '0x22', '0x23', '0x24', '0x25', '0x26', '0x27', '0x28', '0x29', '0x2a', '0x2b', '0x2c', '0x2d', '0x2e', '0x2f', '0x30', '0x31', '0x32', '0x33', '0x34', '0x35', '0x36', '0x37', '0x38', '0x39', '0x3a', '0x3b', '0x3c', '0x3d', '0x3e', '0x3f', '0x40', '0x41', '0x42', '0x43', '0x44', '0x45', '0x46', '0x47', '0x48', '0x49', '0x4a', '0x4b', '0x4c', '0x4d', '0x4e', '0x4f', '0x50', '0x51', '0x52', '0x53', '0x54', '0x55', '0x56', '0x57', '0x58', '0x59', '0x5a', '0x5b', '0x5c', '0x5d', '0x5e', '0x5f', '0x60', '0x61', '0x62', '0x63', '0x64', '0x65', '0x66', '0x67', '0x68', '0x69', '0x6a', '0x6b', '0x6c', '0x6d', '0x6e', '0x6f', '0x70', '0x71', '0x72', '0x73', '0x74', '0x75', '0x76', '0x77', '0x78', '0x79', '0x7a', '0x7b', '0x7c', '0x7d', '0x7e', '0xa1', '0xa2', '0xa3', '0xa4', '0xa5', '0xa6', '0xa7', '0xa8', '0xa9', '0xaa', '0xab', '0xac', '0xae', '0xaf', '0xb0', '0xb1', '0xb2', '0xb3', '0xb4', '0xb5', '0xb6', '0xb7', '0xb8', '0xb9', '0xba', '0xbb', '0xbc', '0xbd', '0xbe', '0xbf', '0xc0', '0xc1', '0xc2', '0xc3', '0xc4', '0xc5', '0xc6', '0xc7', '0xc8', '0xc9', '0xca', '0xcb', '0xcc', '0xcd', '0xce', '0xcf', '0xd0', '0xd1', '0xd2', '0xd3', '0xd4', '0xd5', '0xd6', '0xd7', '0xd8', '0xd9', '0xda', '0xdb', '0xdc', '0xdd', '0xde', '0xdf', '0xe0', '0xe1', '0xe2', '0xe3', '0xe4', '0xe5', '0xe6', '0xe7', '0xe8', '0xe9', '0xea', '0xeb', '0xec', '0xed', '0xee', '0xef', '0xf0', '0xf1', '0xf2', '0xf3', '0xf4', '0xf5', '0xf6', '0xf7', '0xf8', '0xf9', '0xfa', '0xfb', '0xfc', '0xfd', '0xfe', '0xff', '0x100', '0x101', '0x102', '0x103', '0x104', '0x105', '0x106', '0x107', '0x108', '0x109', '0x10a', '0x10b', '0x10c', '0x10d', '0x10e', '0x10f', '0x110', '0x111', '0x112', '0x113', '0x114', '0x115', '0x116', '0x117', '0x118', '0x119', '0x11a', '0x11b', '0x11c', '0x11d', '0x11e', '0x11f', '0x120', '0x121', '0x122', '0x123', '0x124', '0x125', '0x126', '0x127', '0x128', '0x129', '0x12a', '0x12b', '0x12c', '0x12d', '0x12e', '0x12f', '0x130', '0x131', '0x132', '0x133', '0x134', '0x135', '0x136', '0x137', '0x138', '0x139', '0x13a', '0x13b', '0x13c', '0x13d', '0x13e', '0x13f', '0x140', '0x141', '0x142', '0x143']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import pre_tokenizers\n",
    "\n",
    "alphabet = pre_tokenizers.ByteLevel.alphabet()\n",
    "alphabet.sort()\n",
    "print(alphabet)\n",
    "print([ord(i) for i in alphabet])\n",
    "print([hex(ord(i)) for i in alphabet])\n",
    "# ! 一共256个字符，从33开始。因为ascii表中前32个字符是控制字符，所以从33开始。\n",
    "# ascii有128个代码点，unicode的前128个代码点与ascii相同。\n",
    "# ord() 获取字符的 ASCII 码\n",
    "# hex() 将 ASCII 码转换为十六进制字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义特殊标记\n",
    "\n",
    "数据集中存在一些我们不希望被分词的特殊标记，我们会将这些标记定义为特殊标记，并将它们传递给分词器训练器，以防止出现错误的分词情况."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"<unk>\", \"<s>\", \"</s>\"]\n",
    "\n",
    "trainer = trainers.BpeTrainer(\n",
    "        vocab_size=270, # 256 + 14\n",
    "        special_tokens=special_tokens,\n",
    "        show_progress=True,\n",
    "        initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从文件中读取数据\n",
    "\n",
    "在本次实验中，我们使用 JSON Lines（jsonl）格式来存储 Tokenizer 训练数据，分词器内置的训练函数要求训练数据以迭代器的形式传入，因此，我们首先获取一个数据读取的生成器."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_texts_from_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            yield data['text']\n",
    "\n",
    "data_path = './toydata/tokenizer_data.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1: <s>近年来，人工智能技术迅速发展，深刻改变了各行各业的面貌。机器学习、自然语言处理、计算机视觉等领域的突破性进展，使得智能产品和服务越来越普及。从智能家居到自动驾驶，再到智能医疗，AI的应用场景正在快速拓展。随着技术的不断进步，未来的人工智能将更加智能、更加贴近人类生活。</s>\n"
     ]
    }
   ],
   "source": [
    "data_iter = read_texts_from_jsonl(data_path)\n",
    "print(f'Row 1: {next(data_iter)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练!\n",
    "\n",
    "我们使用分词器的内置函数 `tokenizer.train_from_iterator` 来训练分词器."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(data_iter, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，检查一下特殊标记是否得到了妥善处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.token_to_id('<unk>') == 0\n",
    "assert tokenizer.token_to_id('<s>') == 1\n",
    "assert tokenizer.token_to_id('</s>') == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将训练好的分词器保存到磁盘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/toy_tokenizer/vocab.json', './model/toy_tokenizer/merges.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "tokenizer_dir = \"./model/toy_tokenizer\"\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "tokenizer.save(os.path.join(tokenizer_dir, \"tokenizer.json\")) # At this point, you will see a file named tokenizer.json under tokenizer_dir\n",
    "tokenizer.model.save(tokenizer_dir) # generate vocab.json & merges.txt\n",
    "\n",
    "# vocab_size=256+3+11=270，给特殊标记3个，给语料中的pair 11个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手动创建一份配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer training completed and saved.\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"add_bos_token\": False,\n",
    "    \"add_eos_token\": False,\n",
    "    \"add_prefix_space\": False,\n",
    "    \"added_tokens_decoder\": {\n",
    "        \"0\": {\n",
    "            \"content\": \"<unk>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "        },\n",
    "        \"1\": {\n",
    "            \"content\": \"<s>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "        },\n",
    "        \"2\": {\n",
    "            \"content\": \"</s>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "        }\n",
    "    },\n",
    "    \"additional_special_tokens\": [],\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"clean_up_tokenization_spaces\": False,\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"legacy\": True,\n",
    "    \"model_max_length\": 32768,\n",
    "    \"pad_token\": \"<unk>\",\n",
    "    \"sp_model_kwargs\": {},\n",
    "    \"spaces_between_special_tokens\": False,\n",
    "    \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"chat_template\": \"{{ '<s>' + messages[0]['text'] + '</s>' }}\"\n",
    "}\n",
    "\n",
    "with open(os.path.join(tokenizer_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as config_file:\n",
    "    json.dump(config, config_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tokenizer training completed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经训练了一个简单的分词器，并将其进行保存，接下来，我们试着加载它，并使用其帮助我们对文本进行编解码."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hey/anaconda3/envs/minimind/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本：[{'text': '失去的东西就要学着去接受，学着放下。'}]\n",
      "修改文本：[{'text': '失去的东西就要学着去接受，学着放下。'}] (添加自定义聊天模板)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./model/toy_tokenizer\")\n",
    "msg = [{\"text\": \"失去的东西就要学着去接受，学着放下。\"}]\n",
    "new_msg = tokenizer.apply_chat_template(\n",
    "    msg,\n",
    "    tokenize=False\n",
    ")\n",
    "print(f'原始文本：{msg}')\n",
    "print(f'修改文本：{msg} (添加自定义聊天模板)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词器词表大小：270\n"
     ]
    }
   ],
   "source": [
    "print(f'分词器词表大小：{tokenizer.vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看分词结果：\n",
      "{'input_ids': [1, 260, 112, 164, 239, 122, 265, 259, 253, 167, 101, 126, 164, 111, 112, 167, 102, 226, 164, 258, 102, 166, 254, 225, 164, 239, 122, 165, 239, 101, 266, 248, 264, 164, 258, 102, 166, 254, 225, 165, 245, 125, 259, 236, 269, 227, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer(new_msg)\n",
    "print(f'查看分词结果：\\n{model_inputs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对分词结果进行解码：<s>失去的东西就要学着去接受，学着放下。</s> (保留特殊字符)\n"
     ]
    }
   ],
   "source": [
    "response = tokenizer.decode(model_inputs['input_ids'], skip_special_tokens=False)\n",
    "print(f'对分词结果进行解码：{response} (保留特殊字符)' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对分词结果进行解码：失去的东西就要学着去接受，学着放下。 (移除特殊字符)\n"
     ]
    }
   ],
   "source": [
    "response = tokenizer.decode(model_inputs['input_ids'], skip_special_tokens=True)\n",
    "print(f'对分词结果进行解码：{response} (移除特殊字符)' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "\n",
    "- [Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/zh-CN/chapter2/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8/tree/main\n",
    "\n",
    "Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8\n",
    "\n",
    "tokenizer_config.json\n",
    "```json\n",
    "\"model_max_length\": 131072,\n",
    "chat_template=\"\n",
    "{%- if tools %}\n",
    "    {{- '<|im_start|>system\\\\n' }}\n",
    "    {%- if messages[0]['role'] == 'system' %}\n",
    "        {{- messages[0]['content'] }}\n",
    "    {%- else %}\n",
    "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
    "    {%- endif %}\n",
    "    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\n",
    "    {%- for tool in tools %}\n",
    "        {{- \\\"\\\\n\\\" }}\n",
    "        {{- tool | tojson }}\n",
    "    {%- endfor %}\n",
    "    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    \n",
    "    {%- if messages[0]['role'] == 'system' %}\n",
    "        {{- '<|im_start|>system\\\\n' + messages[0]['content'] + '<|im_end|>\\\\n' }}\n",
    "    {%- else %}\n",
    "        {{- '<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n' }}\n",
    "    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\n",
    "    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) or (message.role == \\\"assistant\\\" and not message.tool_calls) %}\n",
    "        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\n",
    "    {%- elif message.role == \\\"assistant\\\" %}\n",
    "        {{- '<|im_start|>' + message.role }}\n",
    "    {%- if message.content %}\n",
    "        {{- '\\\\n' + message.content }}\n",
    "    {%- endif %}\n",
    "    {%- for tool_call in message.tool_calls %}\n",
    "        {%- if tool_call.function is defined %}\n",
    "            {%- set tool_call = tool_call.function %}\n",
    "        {%- endif %}\n",
    "        {{- '\\\\n<tool_call>\\\\n{\\\"name\\\": \\\"' }}\n",
    "        {{- tool_call.name }}\n",
    "        {{- '\\\", \\\"arguments\\\": ' }}\n",
    "        {{- tool_call.arguments | tojson }}\n",
    "        {{- '}\\\\n</tool_call>' }}\n",
    "    {%- endfor %}\n",
    "    {{- '<|im_end|>\\\\n' }}\n",
    "    {%- elif message.role == \\\"tool\\\" %}\n",
    "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\n",
    "            {{- '<|im_start|>user' }}\n",
    "        {%- endif %}\n",
    "        {{- '\\\\n<tool_response>\\\\n' }}\n",
    "        {{- message.content }}\n",
    "        {{- '\\\\n</tool_response>' }}   \n",
    "        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}        \n",
    "            {{- '<|im_end|>\\\\n' }}    \n",
    "        {%- endif %}\n",
    "    {%- endif %}\n",
    "{%- endfor %}\n",
    "{%- if add_generation_prompt %}  \n",
    "    {{- '<|im_start|>assistant\\\\n' }}\n",
    "{%- endif %}\n",
    "   \",\n",
    "```\n",
    "模板的结构分为几个部分。最外层检查是否有 tools 变量存在。如果有的话，生成系统消息，并包含工具的信息。如果没有，则根据 messages 里的第一个消息是否为系统消息来处理。接着遍历 messages 数组，根据不同的角色（user、assistant、tool）生成对应的部分。最后如果有生成提示的话，添加 assistant 的开头。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hey/anaconda3/envs/minimind/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='../model/minimind_tokenizer', vocab_size=6400, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "['<s>', '</s>', '<unk>']\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../model/minimind_tokenizer\")\n",
    "print(tokenizer)\n",
    "print(tokenizer.all_special_tokens)\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['å¤±', 'åİ»', 'çļĦ', 'ä¸ľ', 'è¥¿', 'å°±', 'è¦ģ', 'åŃ¦', 'çĿĢ', 'åİ»', 'æİ¥åıĹ', 'ï¼Į', 'åŃ¦', 'çĿĢ', 'æĶ¾', 'ä¸ĭ', 'ãĢĤ']\n"
     ]
    }
   ],
   "source": [
    "msg = \"失去的东西就要学着去接受，学着放下。\"\n",
    "tokens = tokenizer.tokenize(msg)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2262, 1473, 269, 3600, 2540, 1343, 404, 595, 978, 1473, 4926, 270, 595, 978, 1192, 572, 286]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "失去的东西就要学着去接受，学着放下。\n"
     ]
    }
   ],
   "source": [
    "decoded_str = tokenizer.decode(ids)\n",
    "print(decoded_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{{ '<s>system\\\\n' + system_message + '</s>\\\\n' }}{% else %}{{ '<s>system\\\\n你是 MiniMind，是一个有用的人工智能助手。</s>\\\\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<s>user\\\\n' + content + '</s>\\\\n<s>assistant\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '</s>' + '\\\\n' }}{% endif %}{% endfor %}\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model/minimind_tokenizer/tokenizer_config.json\n",
    "\n",
    "chat_template=\"\n",
    "    {% if messages[0]['role'] == 'system' %}\n",
    "        {% set system_message = messages[0]['content'] %}\n",
    "        {{ '<s>system\\\\n' + system_message + '</s>\\\\n' }}\n",
    "    {% else %}\n",
    "        {{ '<s>system\\\\n你是 MiniMind，是一个有用的人工智能助手。</s>\\\\n' }}\n",
    "    {% endif %}\n",
    "    {% for message in messages %}\n",
    "        {% set content = message['content'] %}\n",
    "        {% if message['role'] == 'user' %}\n",
    "            {{ '<s>user\\\\n' + content + '</s>\\\\n<s>assistant\\\\n' }}\n",
    "        {% elif message['role'] == 'assistant' %}\n",
    "            {{ content + '</s>' + '\\\\n' }}\n",
    "        {% endif %}\n",
    "    {% endfor %}\n",
    "\"\n",
    "\n",
    "<s>system\n",
    "你是 MiniMind，是一个有用的人工智能助手。</s>\n",
    "<s>user\n",
    "content</s>\n",
    "<s>assistant\n",
    "content</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>system\n",
      "你是 xx，是一个有用的人工智能助手。</s>\n",
      "\n",
      "system\n",
      "你是 xx，是一个有用的人工智能助手。\n",
      "\n",
      "<s>system\n",
      "你是 xx，是一个有用的人工智能助手。</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用chat模板\n",
    "msg = [{\"role\": \"system\", \"content\": \"你是 xx，是一个有用的人工智能助手。\"}]\n",
    "new_msg = tokenizer.apply_chat_template(\n",
    "    msg,\n",
    "    tokenize=False\n",
    ")\n",
    "print(new_msg)\n",
    "\n",
    "tokens = tokenizer.tokenize(new_msg)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# 移除特殊字符 # 比如在这个tokenizer中，<s>、</s>、<unk> 是特殊字符\n",
    "decoded_str = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "print(decoded_str)\n",
    "\n",
    "# 保留特殊字符\n",
    "decoded_str = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "print(decoded_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='../model/minimind_tokenizer', vocab_size=6400, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "all_special_tokens:  ['<s>', '</s>', '<unk>']\n",
      "special_tokens_map:  {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}\n",
      "['å¤±', 'åİ»', 'çļĦ', 'ä¸ľ', 'è¥¿', 'å°±', 'è¦ģ', 'åŃ¦', 'çĿĢ', 'åİ»', 'æİ¥åıĹ', 'ï¼Į', 'åŃ¦', 'çĿĢ', 'æĶ¾', 'ä¸ĭ', 'ãĢĤ']\n",
      "['å¤±', 'åİ»', 'çļĦ', 'ä¸ľ', 'è¥¿', 'å°±', 'è¦ģ', 'åŃ¦', 'çĿĢ', 'åİ»', 'æİ¥åıĹ', 'ï¼Į', 'åŃ¦', 'çĿĢ', 'æĶ¾', 'ä¸ĭ', 'ãĢĤ']\n",
      "['ð', 'Ł', 'ĳ', 'į']\n",
      "['ð', 'Ł', 'ĳ', 'į', 'ð', 'Ł', 'ı', '»']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../model/minimind_tokenizer\")\n",
    "print(tokenizer)\n",
    "print(\"all_special_tokens: \", tokenizer.all_special_tokens)\n",
    "print(\"special_tokens_map: \", tokenizer.special_tokens_map)\n",
    "\n",
    "msg = \"失去的东西就要学着去接受，学着放下。\"\n",
    "tokens = tokenizer.tokenize(msg, add_special_tokens=True)\n",
    "print(tokens)\n",
    "tokens = tokenizer.tokenize(msg, add_special_tokens=False)\n",
    "print(tokens)\n",
    "# add_special_tokens=True 会添加特殊字符，add_special_tokens=False 不会添加特殊字符\n",
    "# TODO 没效果，为何没加 <s> 和 </s> 呢？\n",
    "\n",
    "print(tokenizer.tokenize(\"👍\")) # BBPE 会把 emoji 拆分成字节，所以应该是不会有oov和<unk>的\n",
    "print(tokenizer.tokenize(\"👍🏻\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "all_special_tokens:  ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
      "special_tokens_map:  {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "['[CLS]', 'hello', ',', 'world', ',', '[UNK]', '[UNK]', '[UNK]', '文', '西', '!', '[SEP]']\n",
      "['hello', ',', 'world', ',', '[UNK]', '[UNK]', '[UNK]', '文', '西', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# OSError: We couldn't connect to 'https://huggingface.co' to load this file\n",
    "# 需要设置 HF_ENDPOINT=https://hf-mirror.com\n",
    "\n",
    "print(tokenizer)\n",
    "print(\"all_special_tokens: \", tokenizer.all_special_tokens)\n",
    "print(\"special_tokens_map: \", tokenizer.special_tokens_map)\n",
    "\n",
    "msg = \"hello, world, 我是达文西!\"\n",
    "tokens = tokenizer.tokenize(msg, add_special_tokens=True)\n",
    "print(tokens)\n",
    "tokens = tokenizer.tokenize(msg, add_special_tokens=False)\n",
    "print(tokens)\n",
    "# add_special_tokens 有效果了，在tokenization时首尾添加了 [CLS] 和 [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
