{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44db347d-fef7-4e56-aeab-ce55bdab8eab",
   "metadata": {},
   "source": [
    "# 2-Dataset\n",
    "\n",
    "到这里我们便完成了对于 MiniMind Tokenizer 和 Model 部分的全部了解，我们所熟悉的大语言模型正是由这个组件构成的，接下来，我们需要对大模型训练所使用的数据集结构有个基本的认识。\n",
    "\n",
    "想要训练一个能够正常对话，并且符合人类对话偏好的大模型一般需要经过以下几个训练阶段：\n",
    "\n",
    "- 预训练（Pre-training）\n",
    "- 有监督微调（Supervised Fine-tuning，SFT）\n",
    "- 人类反馈强化学习（Reinforcement Learning from Human Feedback，RLHF）\n",
    "\n",
    "在不同训练阶段使用的数据集有所不同，下面会从 MiniMind 代码出发进行介绍和解读。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e41156fd-ca0e-4083-a7a4-a5f98019ab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hey/anaconda3/envs/minimind/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import ast\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d18ad2-3f71-435c-af03-ad9d347aa048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400\n"
     ]
    }
   ],
   "source": [
    "# 从 ../model 目录加载分词器\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('../model/minimind_tokenizer')\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad08fe92-1d04-4b9c-b689-6ad9d763a7c2",
   "metadata": {},
   "source": [
    "## 预训练数据集\n",
    "\n",
    "预训练是模型在大规模语料上进行无监督学习的训练阶段，在该阶段，模型主要学习下一词预测的能力，简单的来说就是学会说话，而不是胡言乱语。因此，该阶段训练的模型不会具有问答能力，而是根据用户输入进行简单的词语接龙。\n",
    "\n",
    "我们可以看一看预训练的数据集格式：\n",
    "\n",
    "```\n",
    "{\"text\": \"如何才能摆脱拖延症？ 治愈拖延症并不容易，但以下建议可能有所帮助...\"}\n",
    "```\n",
    "\n",
    "为了降低该 demo 的运行门槛，在 `./demo` 文件夹下提供了包含两条训练数据的 `pretrain_data.jsonl` 文件作为熟悉训练流程的数据集 demo。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdcac4aa-0ced-4a74-bba0-bc248923c7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1: {'text': 'LLM首先要学习的并非直接与人交流，而是让网络参数中充满知识的墨水，“墨水” 理论上喝的越饱越好，产生大量的对世界的知识积累。 预训练就是让Model先埋头苦学大量基本的知识，例如从Wiki百科、新闻、书籍整理大规模的高质量训练数据。 这个过程是“无监督”的，即人类不需要在过程中做任何“有监督”的校正，而是由模型自己从大量文本中总结规律学习知识点。 模型此阶段目的只有一个：学会词语接龙。例如我们输入“秦始皇”四个字，它可以接龙“是中国的第一位皇帝”。'}\n",
      "\n",
      "Row 2: {'text': '经过预训练，LLM此时已经掌握了大量知识，然而此时它只会无脑地词语接龙，还不会与人聊天。 SFT阶段就需要把半成品LLM施加一个自定义的聊天模板进行微调。 例如模型遇到这样的模板【问题->回答，问题->回答】后不再无脑接龙，而是意识到这是一段完整的对话结束。 称这个过程为指令微调，就如同让已经学富五车的「牛顿」先生适应21世纪智能手机的聊天习惯，学习屏幕左侧是对方消息，右侧是本人消息这个规律。 在训练时，MiniMind的指令和回答长度被截断在512，是为了节省显存空间。就像我们学习时，会先从短的文章开始，当学会写作200字作文后，800字文章也可以手到擒来。 在需要长度拓展时，只需要准备少量的2k/4k/8k长度对话数据进行进一步微调即可（此时最好配合RoPE-NTK的基准差值）。'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 我们可以查看一下 demo 中提供的数据\n",
    "path_pretrain = './toydata/pretrain_data.jsonl'\n",
    "\n",
    "with open(path_pretrain, 'r', encoding='utf-8') as f:\n",
    "    for line_num, line in enumerate(f, 1):\n",
    "        data = json.loads(line.strip())\n",
    "        print(f'Row {line_num}: {data}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0707e3-3ff9-424b-a5be-dab9db55aa44",
   "metadata": {},
   "source": [
    "我们知道，构建一个深度学习数据集需要继承 `torch.utils.data.dataset`，并构建 DataLoader 数据迭代器进行迭代访问。下面，我们来看看 MiniMind 是如何抽象一个预训练数据集的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb2458e3-3661-40de-96be-aa036291a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, max_length=512):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.samples = self.load_data(data_path)\n",
    "\n",
    "    def load_data(self, path):\n",
    "        \"\"\"按行读取 jsonl 文件，并存储在列表中\"\"\"\n",
    "        samples = []\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                data = json.loads(line.strip())\n",
    "                samples.append(data)\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "\n",
    "        text = f\"{self.tokenizer.bos_token}{str(sample['text'])}{self.tokenizer.eos_token}\"\n",
    "        print(text) # uncomment to see formating prompt\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length, # 512\n",
    "            padding='max_length', # 填充到512 # 'longest' 填充到batch中最长序列\n",
    "            truncation=True, # 截断\n",
    "            return_tensors='pt' # 返回tensor\n",
    "        )\n",
    "        print(encoding) # uncomment to see encoding result\n",
    "        input_ids = encoding.input_ids.squeeze() # 压缩维度 # {'input_ids': tensor([[1, 2, 3, 4]])} -> tensor([1, 2, 3, 4])\n",
    "        loss_mask = (input_ids != self.tokenizer.pad_token_id) # mask to ignore loss on pad token\n",
    "\n",
    "        # 嵌入层要求输入是整数类型（torch.long），因为它是通过索引查找对应的向量。若使用浮点类型会直接报错。\n",
    "        # 交叉熵损失（nn.CrossEntropyLoss）的目标（Y）必须是整数类型，表示类别索引\n",
    "        # loss_mask设为torch.long是历史习惯或兼容性处理，但更推荐使用torch.bool或torch.float以提高代码可读性。\n",
    "        X = torch.tensor(input_ids[:-1], dtype=torch.long) # <eos> token not included # torch.long 即 int64\n",
    "        Y = torch.tensor(input_ids[1:], dtype=torch.long) # <bos> token not included\n",
    "        loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long) # align to tensor X\n",
    "        return X, Y, loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "677a1bbb-0df7-45e5-a3ed-d4431cddf43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预训练数据集长度2\n",
      "<s>LLM首先要学习的并非直接与人交流，而是让网络参数中充满知识的墨水，“墨水” 理论上喝的越饱越好，产生大量的对世界的知识积累。 预训练就是让Model先埋头苦学大量基本的知识，例如从Wiki百科、新闻、书籍整理大规模的高质量训练数据。 这个过程是“无监督”的，即人类不需要在过程中做任何“有监督”的校正，而是由模型自己从大量文本中总结规律学习知识点。 模型此阶段目的只有一个：学会词语接龙。例如我们输入“秦始皇”四个字，它可以接龙“是中国的第一位皇帝”。</s>\n",
      "{'input_ids': tensor([[   1,   46,   46,   47, 2105,  404, 1511,  269,  581, 1003, 4656,  850,\n",
      "          415, 3303,  270,  763,  345,  929, 3209, 5832,  413, 3708, 3095,  269,\n",
      "          742,  104,  872,  270, 1024,  742,  104,  872, 1025,  223, 4680,  568,\n",
      "         1080,  254,  269, 1883, 1135,  112, 1883,  587,  270, 2268, 3630,  269,\n",
      "          510, 2219, 6371, 1456, 1591,  825,  110,  286,  223, 1619, 2875, 3541,\n",
      "          929,   47,  565,  392, 1404,  675,  236, 2135, 6301,  595, 3630, 2885,\n",
      "         6371, 1456,  270, 1373, 1083,   57, 3907,   75, 5466, 1493,  341, 5683,\n",
      "          341, 2170, 6357, 1354,  578,  611, 1379, 1605,  269,  654, 1837, 2875,\n",
      "         1327,  286,  223,  990, 1629,  345, 1024, 1308, 3608,  878,   99, 1025,\n",
      "          269,  270, 1933, 1897,  453,  590,  368, 2658, 1571, 2870, 1024,  400,\n",
      "         3608,  878,   99, 1025, 3268,   97, 1197,  270,  763, 4856, 2933,  867,\n",
      "         1083, 3630, 1966,  413, 3650, 1379, 2127, 1511, 3095,  829,  286,  223,\n",
      "         2933,  856, 5578, 5383, 4134,  541,  355, 5107, 1582,  965, 1468, 4564,\n",
      "          286, 1373,  551, 2346, 1024,  441,  102, 1696,  268,  232, 1025, 2754,\n",
      "          371, 1129,  270,  701,  451, 1468, 4564, 1024,  345, 2366, 5221,  108,\n",
      "         4374,  268,  232,  394,  254, 4383,    2,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "torch.Size([511]) torch.Size([511]) torch.Size([511])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10254/3782234284.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(input_ids[:-1], dtype=torch.long) # <eos> token not included # torch.long 即 int64\n",
      "/tmp/ipykernel_10254/3782234284.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y = torch.tensor(input_ids[1:], dtype=torch.long) # <bos> token not included\n",
      "/tmp/ipykernel_10254/3782234284.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long) # align to tensor X\n"
     ]
    }
   ],
   "source": [
    "pretrain_dataset = PretrainDataset(path_pretrain, tokenizer)\n",
    "print(f'预训练数据集长度{len(pretrain_dataset)}')\n",
    "x, y, lm = pretrain_dataset[0]\n",
    "print(x.shape, y.shape, lm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb160cb-0e96-45a7-a1c0-be145895e70f",
   "metadata": {},
   "source": [
    "## 有监督微调数据集\n",
    "\n",
    "有监督微调（Supervised Fine Tuning，SFT）对预训练后得到的基座 LLM 施加一个自定义聊天模板进行微调，由于在这一阶段，模型训练的目标是根据用户指令生成响应（构建问答体系），故又称为指令微调。\n",
    "\n",
    "我们可以看一看有监督微调的数据集格式：\n",
    "\n",
    "```\n",
    "{\n",
    "    \"conversations\": [\n",
    "        {\"role\": \"user\", \"content\": \"你好\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"你好！\"},\n",
    "        {\"role\": \"user\", \"content\": \"再见\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"再见！\"}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "为了降低该 demo 的运行门槛，在 `./demo` 文件夹下提供了包含两条 conversation 问答数据的 `sft_data.jsonl` 文件作为熟悉训练流程的数据集 demo。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e025e25-c937-4468-ae66-b32447440658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1: {'conversations': [{'role': 'user', 'content': '你好吗？'}, {'role': 'assistant', 'content': '我很好，谢谢！你呢？'}, {'role': 'user', 'content': '我也很好，谢谢！'}, {'role': 'assistant', 'content': '太好了！祝你今天愉快！'}]}\n",
      "\n",
      "Row 2: {'conversations': [{'role': 'user', 'content': '你喜欢什么运动？'}, {'role': 'assistant', 'content': '我喜欢跑步和游泳。你呢？'}, {'role': 'user', 'content': '我喜欢打篮球！'}, {'role': 'assistant', 'content': '篮球很棒！是一个很好的团队运动。'}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 我们可以查看一下 demo 中提供的数据\n",
    "path_sft = './toydata/sft_data.jsonl'\n",
    "\n",
    "with open(path_sft, 'r', encoding='utf-8') as f:\n",
    "    for line_num, line in enumerate(f, 1):\n",
    "        data = json.loads(line.strip())\n",
    "        print(f'Row {line_num}: {data}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f988233-530f-4ddf-8d64-9532442a2535",
   "metadata": {},
   "source": [
    "接下来，我们尝试构造一个数据集对象，实现对 sft 格式数据的读取与处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c636bc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1078, 538, 501, 201]\n",
      "[1, 1078, 538, 501, 201]\n"
     ]
    }
   ],
   "source": [
    "# text转token\n",
    "# add_special_tokens：自动添加模型预定义的特殊标记\n",
    "'''\n",
    "常见的特殊标记：\n",
    "BERT 系列：[CLS]（分类标记，放在开头）、[SEP]（分隔句子或结尾）\n",
    "GPT 系列：<s>（开始）、</s>（结束）\n",
    "其他模型：如<pad>（填充）、<mask>（掩码）、<unk>（未知词）等\n",
    "'''\n",
    "\n",
    "print(tokenizer('<s>assistant\\n', add_special_tokens=True).input_ids)\n",
    "print(tokenizer('<s>assistant\\n', add_special_tokens=False).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b25f0c6-b5b5-47b1-974f-15a2f0d2760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, tokenizer, max_length=512):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.samples = self.load_data(jsonl_path)\n",
    "        self.bos_id = tokenizer('<s>assistant\\n', add_special_tokens=False).input_ids # set bos token\n",
    "        self.eos_id = tokenizer('</s>\\n', add_special_tokens=False).input_ids # set eos token\n",
    "\n",
    "    def load_data(self, path):\n",
    "        samples = []\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                data = json.loads(line.strip())\n",
    "                samples.append(data)\n",
    "        return samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _create_chat_prompt(self, conversations):\n",
    "        \"\"\"构建符合 ChatML 格式的对话\"\"\"\n",
    "        messages = []\n",
    "        for i, turn in enumerate(conversations): # for each speaker in one conversation\n",
    "            role = 'user' if i % 2 == 0 else 'assistant'\n",
    "            messages.append({\"role\": role, \"content\": turn['content']})\n",
    "        return self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "\n",
    "    def _generate_loss_mask(self, input_ids):\n",
    "        loss_mask = [0] * len(input_ids)\n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            if input_ids[i:i + len(self.bos_id)] == self.bos_id: # check if reach bos token\n",
    "                ########### find content start & end point ###########\n",
    "                start = i + len(self.bos_id) # set start point at the end of bos token\n",
    "                end = start\n",
    "                while end < len(input_ids):\n",
    "                    if input_ids[end:end + len(self.eos_id)] == self.eos_id: # check if reach eos token\n",
    "                        break\n",
    "                    end += 1\n",
    "                ######################################################\n",
    "                for j in range(start + 1, min(end + len(self.eos_id) + 1, self.max_length)): # ignore tokens that reach max input length\n",
    "                    loss_mask[j] = 1\n",
    "                i = end + len(self.eos_id) if end < len(input_ids) else len(input_ids) # update i to exit current conversation turn\n",
    "            else:\n",
    "                i += 1\n",
    "        return loss_mask\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        print(\"sample: \", sample)\n",
    "        prompt = self._create_chat_prompt(sample['conversations'])\n",
    "        print(\"prompt: \", prompt)\n",
    "        # print(prompt) # uncomment to see formating prompt\n",
    "        tokens = self.tokenizer.tokenize(prompt)\n",
    "        # ids = self.tokenizer.convert_tokens_to_string\n",
    "        input_ids = self.tokenizer(prompt).input_ids[:self.max_length] # encode input\n",
    "        input_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(input_ids)) # encode padding\n",
    "        # print(input_ids) # uncomment to see encoded prompt\n",
    "        \n",
    "        loss_mask = self._generate_loss_mask(input_ids)\n",
    "\n",
    "        X = torch.tensor(input_ids[:-1], dtype=torch.long)\n",
    "        Y = torch.tensor(input_ids[1:], dtype=torch.long)\n",
    "        loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long) # align with pred pos\n",
    "\n",
    "        print(\"loss_mask=1所对应的text:\")\n",
    "        text = []\n",
    "        for i, j in zip(tokens[:self.max_length-1], loss_mask):\n",
    "            if j:\n",
    "                text.append(i)\n",
    "        print(self.tokenizer.convert_tokens_to_string(text))\n",
    "\n",
    "        return X, Y, loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67a62dbf-6a50-4faf-8ad1-5228eb8aa7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "sample:  {'conversations': [{'role': 'user', 'content': '你好吗？'}, {'role': 'assistant', 'content': '我很好，谢谢！你呢？'}, {'role': 'user', 'content': '我也很好，谢谢！'}, {'role': 'assistant', 'content': '太好了！祝你今天愉快！'}]}\n",
      "prompt:  <s>system\n",
      "你是 MiniMind，是一个有用的人工智能助手。</s>\n",
      "<s>user\n",
      "你好吗？</s>\n",
      "<s>assistant\n",
      "我很好，谢谢！你呢？</s>\n",
      "<s>user\n",
      "我也很好，谢谢！</s>\n",
      "<s>assistant\n",
      "太好了！祝你今天愉快！</s>\n",
      "\n",
      "loss_mask=1所对应的text:\n",
      "我很好，谢谢！你呢？</s>\n",
      "太好了！祝你今天愉快！</s>\n",
      "\n",
      "样本 shape = torch.Size([511]), 标签 shape = torch.Size([511]), loss_mask shape torch.Size([511])\n"
     ]
    }
   ],
   "source": [
    "sft_dataset = SFTDataset(path_sft, tokenizer)\n",
    "print(len(sft_dataset))\n",
    "x, y, lm = sft_dataset[0]\n",
    "print(f'样本 shape = {x.shape}, 标签 shape = {y.shape}, loss_mask shape {lm.shape}')\n",
    "# print(lm) # 打印 loss mask，你会发现在序列中有两处以 1 填充的序列，这是因为我们在一个 conversation 中开展了两轮对话，其中只有 assistant 回复计算损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7094ff6-4657-4483-8c34-785ffe9adbd4",
   "metadata": {},
   "source": [
    "## 人类反馈强化学习数据集\n",
    "\n",
    "在 MiniMind 项目中，采用直接偏好优化（Direct Preference Optimization，DPO）训练大模型对齐人类偏好。在这一训练阶段，模型将会根据提供的问答正反例进行偏好优化，从而降低让人类不满意的答案出现的几率。\n",
    "\n",
    "与PPO(Proximal Policy Optimization)这种需要奖励模型、价值模型的RL算法不同； DPO通过推导PPO奖励模型的显式解，把在线奖励模型换成离线数据，Ref模型输出可以提前保存。 DPO性能几乎不变，只用跑 actor_model 和 ref_model 两个模型，大大节省显存开销和增加训练稳定性。\n",
    "\n",
    "我们可以看一看DPO的数据集格式：\n",
    "\n",
    "```\n",
    "{\n",
    "  \"chosen\": [\n",
    "    {\"content\": \"Query\", \"role\": \"user\"}, \n",
    "    {\"content\": \"good answer\", \"role\": \"assistant\"}\n",
    "  ], \n",
    "  \"rejected\": [\n",
    "    {\"content\": \"Query\", \"role\": \"user\"}, \n",
    "    {\"content\": \"bad answer\", \"role\": \"assistant\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "为了降低该 demo 的运行门槛，在 ./demo 文件夹下提供了包含两条 问答正反例 数据的 dpo_data.jsonl 文件作为熟悉训练流程的数据集 demo。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81682b5f-0d3e-4fa2-aec4-0be2b19d5c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1: {'chosen': [{'content': '你好吗？', 'role': 'user'}, {'content': '我很好，谢谢！你呢？', 'role': 'assistant'}, {'content': '今天过得怎么样？', 'role': 'user'}, {'content': '挺好的，去跑步了，心情不错。', 'role': 'assistant'}], 'rejected': [{'content': '你好吗？', 'role': 'user'}, {'content': '不好，我很累。', 'role': 'assistant'}, {'content': '你喜欢什么运动？', 'role': 'user'}, {'content': '我不喜欢运动，没兴趣。', 'role': 'assistant'}]}\n",
      "\n",
      "Row 2: {'chosen': [{'content': '你喜欢旅行吗？', 'role': 'user'}, {'content': '我喜欢，尤其是去海边。', 'role': 'assistant'}, {'content': '你去过哪些地方？', 'role': 'user'}, {'content': '我去过欧洲、美国和日本，都是很棒的经历。', 'role': 'assistant'}], 'rejected': [{'content': '你喜欢旅行吗？', 'role': 'user'}, {'content': '不太喜欢，觉得太麻烦。', 'role': 'assistant'}, {'content': '你去过哪些地方？', 'role': 'user'}, {'content': '没去过太多地方，主要是待在家里。', 'role': 'assistant'}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 我们可以查看一下 demo 中提供的数据\n",
    "path_dpo = './toydata/dpo_data.jsonl'\n",
    "\n",
    "with open(path_dpo, 'r', encoding='utf-8') as f:\n",
    "    for line_num, line in enumerate(f, 1):\n",
    "        data = json.loads(line.strip())\n",
    "        print(f'Row {line_num}: {data}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5446a192",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    'chosen': [\n",
    "        {'content': '你好吗？', 'role': 'user'}, \n",
    "        {'content': '我很好，谢谢！你呢？', 'role': 'assistant'}, \n",
    "        {'content': '今天过得怎么样？', 'role': 'user'}, \n",
    "        {'content': '挺好的，去跑步了，心情不错。', 'role': 'assistant'}\n",
    "    ], \n",
    "    'rejected': [\n",
    "        {'content': '你好吗？', 'role': 'user'}, \n",
    "        {'content': '不好，我很累。', 'role': 'assistant'}, \n",
    "        {'content': '你喜欢什么运动？', 'role': 'user'}, \n",
    "        {'content': '我不喜欢运动，没兴趣。', 'role': 'assistant'}\n",
    "    ]}\n",
    "```\n",
    "```json\n",
    "{\n",
    "    'chosen': [\n",
    "        {'content': '你喜欢旅行吗？', 'role': 'user'}, \n",
    "        {'content': '我喜欢，尤其是去海边。', 'role': 'assistant'}, \n",
    "        {'content': '你去过哪些地方？', 'role': 'user'}, \n",
    "        {'content': '我去过欧洲、美国和日本，都是很棒的经历。', 'role': 'assistant'}\n",
    "    ], \n",
    "    'rejected': [\n",
    "        {'content': '你喜欢旅行吗？', 'role': 'user'}, \n",
    "        {'content': '不太喜欢，觉得太麻烦。', 'role': 'assistant'}, \n",
    "        {'content': '你去过哪些地方？', 'role': 'user'}, \n",
    "        {'content': '没去过太多地方，主要是待在家里。', 'role': 'assistant'}\n",
    "    ]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb54557-54f4-4004-8a7b-443b8671002b",
   "metadata": {},
   "source": [
    "接下来，我们尝试构造 json 对象，实现对 dpo 格式数据的读取和处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efa98777-5d3a-4b3f-a9fb-e0f05e4af81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPODataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=512):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.padding = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "        self.bos_id = tokenizer('<s>assistant\\n', add_special_tokens=False).input_ids # content prefix\n",
    "        self.eos_id = tokenizer('</s>\\n', add_special_tokens=False).input_ids # content suffix\n",
    "        with open(file_path, 'r', encoding='utf-8') as f: # load data\n",
    "            self.data = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                obj = json.loads(line)\n",
    "                self.data.append(obj)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _generate_loss_mask(self, input_ids):\n",
    "        \"\"\"此处的损失掩码生成函数与 SFT 阶段逻辑一致\"\"\"\n",
    "        loss_mask = [0] * len(input_ids)\n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            if input_ids[i:i + len(self.bos_id)] == self.bos_id:\n",
    "                start = i + len(self.bos_id)\n",
    "                end = start\n",
    "                while end < len(input_ids):\n",
    "                    if input_ids[end:end + len(self.eos_id)] == self.eos_id:\n",
    "                        break\n",
    "                    end += 1\n",
    "                for j in range(start + 1, min(end + len(self.eos_id) + 1, self.max_length)):\n",
    "                    loss_mask[j] = 1\n",
    "                i = end + len(self.eos_id) if end < len(input_ids) else len(input_ids)\n",
    "            else:\n",
    "                i += 1\n",
    "        return loss_mask\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        chosen = item['chosen'] # 一个 list，里面包含若干 {role, content}\n",
    "        rejected = item['rejected'] # 同上\n",
    "        # format prompt\n",
    "        chosen_prompt = self.tokenizer.apply_chat_template(\n",
    "            chosen, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        print(chosen_prompt) # uncomment to see formating prompt\n",
    "        rejected_prompt = self.tokenizer.apply_chat_template(\n",
    "            rejected, tokenize = False, add_gerneration_prompt=False\n",
    "        )\n",
    "        print(rejected_prompt) # uncomment to see formating prompt\n",
    "        # tokenize\n",
    "        chosen_encoding = self.tokenizer(\n",
    "            chosen_prompt, truncation=True, max_length=self.max_length, padding='max_length'\n",
    "        )\n",
    "        rejected_encoding = self.tokenizer(\n",
    "            rejected_prompt, truncation=True, max_length=self.max_length, padding='max_length'\n",
    "        )\n",
    "        # generate loss mask\n",
    "        chosen_input_ids = chosen_encoding['input_ids']\n",
    "        chosen_loss_mask = self._generate_loss_mask(chosen_input_ids)\n",
    "        rejected_input_ids = rejected_encoding['input_ids']\n",
    "        rejected_loss_mask = self._generate_loss_mask(rejected_input_ids)\n",
    "        # same as sft / pretrain\n",
    "        x_chosen = torch.tensor(chosen_input_ids[:-1], dtype=torch.long)\n",
    "        y_chosen = torch.tensor(chosen_input_ids[1:], dtype=torch.long)\n",
    "        mask_chosen = torch.tensor(chosen_loss_mask[1:], dtype=torch.long)\n",
    "        x_rejected = torch.tensor(rejected_input_ids[:-1], dtype=torch.long)\n",
    "        y_rejected = torch.tensor(rejected_input_ids[1:], dtype=torch.long)\n",
    "        mask_rejected = torch.tensor(rejected_loss_mask[1:], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'x_chosen': x_chosen,\n",
    "            'y_chosen': y_chosen,\n",
    "            'mask_chosen': mask_chosen,\n",
    "            'x_rejected': x_rejected,\n",
    "            'y_rejected': y_rejected,\n",
    "            'mask_rejected': mask_rejected\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ba205c4-0dd2-48fd-8aa2-26a42afc90d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO 数据集长度：2\n",
      "<s>system\n",
      "你是 MiniMind，是一个有用的人工智能助手。</s>\n",
      "<s>user\n",
      "你好吗？</s>\n",
      "<s>assistant\n",
      "我很好，谢谢！你呢？</s>\n",
      "<s>user\n",
      "今天过得怎么样？</s>\n",
      "<s>assistant\n",
      "挺好的，去跑步了，心情不错。</s>\n",
      "\n",
      "<s>system\n",
      "你是 MiniMind，是一个有用的人工智能助手。</s>\n",
      "<s>user\n",
      "你好吗？</s>\n",
      "<s>assistant\n",
      "不好，我很累。</s>\n",
      "<s>user\n",
      "你喜欢什么运动？</s>\n",
      "<s>assistant\n",
      "我不喜欢运动，没兴趣。</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dpo_dataset = DPODataset(path_dpo, tokenizer)\n",
    "print(f'DPO 数据集长度：{len(dpo_dataset)}')\n",
    "res = dpo_dataset[0]\n",
    "# 如有需要，请自主查看 res 中的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58640448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
